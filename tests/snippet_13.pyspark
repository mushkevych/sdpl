# File generated by SDPL compiler from tests/snippet_13.sdpl
# Do not edit the file manually
# # snippet: load data from all supported types of sources, store data in all supported types of sink
A = sqlContext.read.csv('s3://my_bucket.s3.amazonaws.com:443/path/within/bucket/table_name', schema=StructType([
    StructField('a', StringType, False),
    StructField('aa', StringType, False),
    StructField('aaa', StringType, False),
    StructField('column', BooleanType, False),
    StructField('another_column', BooleanType, False),
    StructField('yet_another_column', BooleanType, False) ])
    , sep=',')
B = sqlContext.read.csv('/data/hive/db_name/file_blob', schema=StructType([
    StructField('b', IntegerType, False),
    StructField('bb', IntegerType, False),
    StructField('bbb', IntegerType, False),
    StructField('column', BooleanType, False),
    StructField('another_column', BooleanType, False),
    StructField('yet_another_column', BooleanType, False) ])
    , sep='\t')
JOIN_A_B = A.join(B, (A.a == B.b) & (A.aa == B.bb), 'left_outer')
Z = JOIN_A_B.select(
    col('a').alias('a'),
    col('aa').alias('aa'),
    col('aaa').alias('aaa'),
    col('column').alias('column'),
    col('another_column').alias('another_column'),
    col('yet_another_column').alias('yet_another_column'),
    col('b').alias('b'),
    col('bb').alias('bb'),
    col('bbb').alias('bbb')
)
sqlContext.write.jdbc(jdbc:postgresql://host.the_company.xyz:6789/mydb, db_schema.db_table_name, properties={ 'user': 'the_user', 'password': 'the_password' })
sqlContext.write.csv('s3://my_bucket.s3.amazonaws.com:443/path/within/bucket/my_file.gz', compression='SNAPPY', sep=',')
sqlContext.write.csv('/data/hive/db_name/my_file', compression='NONE', sep='\t')
# SDPL output: EOF
