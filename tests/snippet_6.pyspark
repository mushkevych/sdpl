# File generated by SDPL compiler from tests/snippet_6.sdpl
# Do not edit the file manually
# # snippet: multi-relational joins
A = sqlContext.read.csv('s3://my_bucket.s3.amazonaws.com:443/path/within/bucket/table_name', schema=StructType([
    StructField('a', StringType, False),
    StructField('aa', StringType, False),
    StructField('aaa', StringType, False),
    StructField('column', BooleanType, False),
    StructField('another_column', BooleanType, False),
    StructField('yet_another_column', BooleanType, False) ])
    , sep=',')
B = sqlContext.read.csv('s3://my_bucket.s3.amazonaws.com:443/path/within/bucket/file_blob', schema=StructType([
    StructField('b', IntegerType, False),
    StructField('bb', IntegerType, False),
    StructField('bbb', IntegerType, False),
    StructField('column', BooleanType, False),
    StructField('another_column', BooleanType, False),
    StructField('yet_another_column', BooleanType, False) ])
    , sep=',')
C = A.filter((col(A.a) == 3) & ((col(A.aa) >= 0) | (col(A.aaa) < -100)))
D = B.filter((col(B.b) != 'Bebe') | (col(B.bb) == 'Zeze'))
# quoted source code: start
X = LOAD 'data' AS (a1:int,a2:int,a3:int);
DUMP X;
Y = FILTER X BY a3 == 3;

STORE X INTO 'db://db_connection_string';
STORE Y INTO 'output' USING PigDump();
# quoted source code: finish
sqlContext.write.csv('s3://my_bucket.s3.amazonaws.com:443/path/within/bucket/table_name_c', compression='SNAPPY', sep=',')
sqlContext.write.csv('s3://my_bucket.s3.amazonaws.com:443/path/within/bucket/table_name_d', compression='SNAPPY', sep=',')
# SDPL output: EOF
